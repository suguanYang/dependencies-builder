we should make the LLM load files on demand if the context is overwhelming.
However, to maintain the stability you require, you cannot just leave it up to the LLM's "whim". You must implement a strict "Triage â†’ Fetch â†’ Analyze" workflow.
Here are the two best architecture patterns to solve "Context Overflow" while maintaining "Result Stability."
Solution 1: The "Two-Step Agent" (On-Demand Loading)
This is the most token-efficient method. You split the process into two distinct LLM calls (or a loop).
Step 1: Triage (The "Is it worth checking?" step)
Send ONLY the Diff and the Dependency Metadata (File path, Line number, Type) to the LLM. Do not send any file content.
Prompt 1 (Triage):
code
Markdown
# Task
Review the provided [Source Code Diff].
Determine if this change requires inspecting the dependent files.

# Rules
1. If the Diff is **COSMETIC ONLY** (whitespace, comments, pure formatting):
   - Return status: "SKIP"
2. If the Diff changes **LOGIC, TYPES, or SIGNATURES**:
   - Return status: "FETCH_REQUIRED"
   - List the IDs of the files we need to fetch.

# Input
<diff>...</diff>
<dependency_list>
  <item id="1">Path: src/app.tsx, Line: 50</item>
  <item id="2">Path: src/utils.ts, Line: 10</item>
</dependency_list>
Step 2: Execution (The "Analysis" step)
If Step 1 says "FETCH_REQUIRED", your system (Node.js/Python) performs the get_file_contents for the specific files requested. Then, you send a second prompt with the content.
Prompt 2 (Analysis):
code
Markdown
# Context
Diff: [Insert Diff again]
File Contents: [Insert the content you just fetched]

# Task
Analyze impact based on the code provided...
Why this is stable:
You force the LLM to filter noise before wasting tokens on reading files.
You verify the logic after getting the files.
Solution 2: The "Smart Window" (Pre-fetch Optimization)
If you prefer the single-request "Pre-fetch" approach for stability but want to reduce tokens, do not read the full file.
Code files can be 2,000 lines long, but the dependency usage usually only matters within a +/- 50 line window.
Modify your get_file_contents tool logic:
Instead of:
code
TypeScript
// Bad: Loads entire 50kb file
const content = fs.readFileSync(filePath, 'utf-8');
Do this:
code
TypeScript
// Good: Loads only the relevant "Context Window"
function getContextWindow(filePath, targetLine, windowSize = 50) {
  const content = fs.readFileSync(filePath, 'utf-8');
  const lines = content.split('\n');
  
  const start = Math.max(0, targetLine - windowSize);
  const end = Math.min(lines.length, targetLine + windowSize);
  
  return lines.slice(start, end).join('\n');
}
Why this is stable:
It reduces token usage by ~90% (assuming huge files).
It keeps the "Anchor" logic (the LLM still sees exactly how the code is used).
It fits many more dependencies into a single context window.
Solution 3: The "Map-Reduce" Batching (For Massive Scale)
If you have 50 affected projects, even "Smart Windows" will overflow the context. You must break the request into batches.
Algorithm:
Group the affected dependencies into chunks of 5 or 10.
Parallel Process: Send 5 concurrent requests to the LLM.
Request A: Analyze Projects 1-5
Request B: Analyze Projects 6-10
...
Reduce: Collect all the JSON outputs and merge them into one final report array.
Recommended Implementation (Best Balance)
I recommend Solution 2 (Smart Window) combined with Solution 3 (Batching). It provides the highest stability because the LLM doesn't need to "decide" to call tools (which can fail), but it keeps tokens manageable.
Prompt for Smart Window Strategy:
code
Markdown
**Context Data:**
<provider_change>
  {git_diff}
</provider_change>

<consumer_files>
  <file index="1">
    <path>src/pages/detail.tsx</path>
    <usage_line>150</usage_line>
    <content_snippet>
      <!-- Insert lines 100 to 200 here -->
      ...
      149: // Prep arguments
      150: const res = await changedFunction(arg1, arg2); // <--- ANCHOR
      151: if (res.error) handle(res);
      ...
    </content_snippet>
  </file>
  <!-- Repeat for other files -->
</consumer_files>

**Instruction:**
Analyze the <provider_change>. If it is not cosmetic, check the <content_snippet> around line <usage_line>.
Note: The content provided is a truncated window of the full file. Focus analysis on the visible logic flow.



ä½†æ˜¯æˆ‘ä»¬ä¸ºäº†æ—¢èƒ½ä¿è¯**ç¨³å®šæ€§**ï¼ˆå°½å¯èƒ½ä½¿ç”¨å…¨é‡æ–‡ä»¶å†…å®¹ï¼‰ï¼Œåˆèƒ½é˜²æ­¢**Token æº¢å‡º**ï¼Œä½ éœ€è¦åœ¨è¿™ä¸ª Node.js/CLI ç¨‹åºä¸­å®ç°ä¸€ä¸ª**"åŠ¨æ€ä¸Šä¸‹æ–‡æ„å»ºå™¨" (Dynamic Context Builder)**ã€‚

æ ¸å¿ƒæ€è·¯æ˜¯ï¼š**åœ¨å‘é€è¯·æ±‚ç»™ LLM ä¹‹å‰ï¼Œå…ˆåœ¨æœ¬åœ°è®¡ç®— Token å¼€é”€ï¼Œæ ¹æ®å¼€é”€å¤§å°åŠ¨æ€å†³å®šç­–ç•¥ã€‚**

ä»¥ä¸‹æ˜¯å…·ä½“çš„å®ç°æ–¹æ¡ˆå’Œç®—æ³•é€»è¾‘ï¼š

### 1. æ ¸å¿ƒé€»è¾‘ï¼šToken é¢„ç®—ç®¡ç† (Token Budget Manager)

ä½ éœ€è¦å¼•å…¥ä¸€ä¸ªç®€å•çš„ Token è®¡ç®—ä¼°ç®—å™¨ï¼ˆæ¯”å¦‚ `tiktoken` æˆ–è€…ç®€å•çš„ `char_length / 4`ï¼‰ï¼Œç„¶åæŒ‰ç…§ä»¥ä¸‹ä¼˜å…ˆçº§è¿›è¡Œé™çº§ï¼š

1.  **ä¼˜å…ˆçº§ A (ç†æƒ³æƒ…å†µ):** å…¨é‡å‘é€æ‰€æœ‰ä¾èµ–æ–‡ä»¶å®Œæ•´å†…å®¹ï¼ˆä½ çš„é¦–é€‰ï¼‰ã€‚
2.  **ä¼˜å…ˆçº§ B (å½“æ€»é‡è¿‡å¤§æ—¶):** ä¿æŒå…¨é‡å†…å®¹ï¼Œä½†å°†ä»»åŠ¡**åˆ†æ‰¹ (Batching)** å‘é€å¤šæ¬¡ã€‚
3.  **ä¼˜å…ˆçº§ C (å½“å•æ–‡ä»¶è¿‡å¤§æ—¶):** å¯¹è¶…å¤§æ–‡ä»¶ä½¿ç”¨ **"æ™ºèƒ½çª—å£" (Smart Window)** åˆ‡ç‰‡ã€‚

---

### 2. ä»£ç å®ç°ç¤ºä¾‹ (TypeScript)

å‡è®¾ä½ æ­£åœ¨ä½¿ç”¨ `src/index.ts`ï¼Œä½ å¯ä»¥å°è£…ä¸€ä¸ª `ContextBuilder` ç±»ã€‚

ä½ éœ€è¦å®‰è£… `tiktoken` (æˆ–è€…ä½¿ç”¨ç®€å•çš„é•¿åº¦ä¼°ç®—) æ¥è®¡ç®— Tokenã€‚

```typescript
import { get_encoding } from "tiktoken";
import fs from 'fs';

// 1. å®šä¹‰ä½ çš„æ¨¡å‹é™åˆ¶ (ç•™å‡º 20% ç»™ output å’Œ system prompt)
const MODEL_MAX_TOKENS = 128000; 
const SAFE_BUFFER = 4000; // ç»™ Output é¢„ç•™
const SYSTEM_PROMPT_COST = 2000; // é¢„ä¼° System Prompt å¤§å°
const AVAILABLE_TOKENS = MODEL_MAX_TOKENS - SAFE_BUFFER - SYSTEM_PROMPT_COST;

const encoder = get_encoding("cl100k_base"); // GPT-4 çš„ tokenizer

interface DependencyItem {
  project: string;
  filePath: string;
  line: number;
  // ... å…¶ä»–å…ƒæ•°æ®
}

// è¾…åŠ©å‡½æ•°ï¼šè®¡ç®— Token
function countTokens(text: string): number {
  return encoder.encode(text).length;
}

// æ ¸å¿ƒå¤„ç†å‡½æ•°
async function processDependencies(
  diffContent: string, 
  dependencies: DependencyItem[]
) {
  // 1. è®¡ç®—"å›ºå®šå¼€é”€" (Diff + Prompt æ¨¡æ¿)
  const diffTokens = countTokens(diffContent);
  let currentTokenUsage = diffTokens;
  
  // 2. å‡†å¤‡æ‰¹æ¬¡å®¹å™¨
  let batches: any[] = [];
  let currentBatch: any[] = [];
  
  // 3. éå†æ¯ä¸€ä¸ªä¾èµ–æ–‡ä»¶ï¼Œå†³å®šå¦‚ä½•åŠ è½½
  for (const dep of dependencies) {
    let fileContent = "";
    try {
      fileContent = fs.readFileSync(dep.filePath, 'utf-8');
    } catch (e) {
      fileContent = "[System Error: File Not Found]";
    }

    const fileTokens = countTokens(fileContent);

    // --- ç­–ç•¥åˆ¤å®šç‚¹ ---

    // æƒ…å†µ A: å•ä¸ªæ–‡ä»¶ç”šè‡³æ¯”æ€»é™åˆ¶è¿˜å¤§ï¼Ÿ(æå°‘è§ï¼Œä½†è¦é˜²å®ˆ)
    if (fileTokens > AVAILABLE_TOKENS - diffTokens) {
      console.warn(`âš ï¸ æ–‡ä»¶ ${dep.filePath} å¤ªå¤§ (${fileTokens} tokens)ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸º Smart Window æ¨¡å¼`);
      // ç­–ç•¥ C: åªå–ç›¸å…³è¡Œé™„è¿‘ 100 è¡Œ
      fileContent = getSmartWindow(fileContent, dep.line, 100); 
    }

    // è®¡ç®—åŠ å…¥è¯¥æ–‡ä»¶åæ˜¯å¦ä¼šæº¢å‡ºå½“å‰æ‰¹æ¬¡
    const nextTotal = currentTokenUsage + countTokens(fileContent) + 50; // +50 æ˜¯ XML æ ‡ç­¾çš„å¼€é”€

    // æƒ…å†µ B: å½“å‰æ‰¹æ¬¡æ»¡äº†ï¼Œéœ€è¦å¼€å¯æ–°æ‰¹æ¬¡
    if (nextTotal > AVAILABLE_TOKENS) {
      console.log(`ğŸ“¦ æ‰¹æ¬¡å·²æ»¡ï¼Œå¼€å¯æ–°æ‰¹æ¬¡...`);
      batches.push(currentBatch);
      currentBatch = [];
      currentTokenUsage = diffTokens; // æ–°æ‰¹æ¬¡ä¹Ÿè¦åŒ…å« Diff
    }

    // åŠ å…¥å½“å‰æ‰¹æ¬¡ (é»˜è®¤ç­–ç•¥ A: å…¨é‡å†…å®¹)
    currentBatch.push({
      metadata: dep,
      content: fileContent
    });
    currentTokenUsage += countTokens(fileContent);
  }

  // æ¨å…¥æœ€åä¸€ä¸ªæ‰¹æ¬¡
  if (currentBatch.length > 0) {
    batches.push(currentBatch);
  }

  // 4. æ‰§è¡Œ LLM è¯·æ±‚ (Map-Reduce)
  console.log(`ğŸš€ æ€»å…±åˆ†ä¸º ${batches.length} ä¸ªè¯·æ±‚å¹¶è¡Œåˆ†æ`);
  
  const results = await Promise.all(batches.map(batch => callLLM(diffContent, batch)));
  
  // 5. åˆå¹¶ç»“æœ
  return mergeResults(results);
}

// è¾…åŠ©å‡½æ•°ï¼šæ™ºèƒ½çª—å£åˆ‡ç‰‡ (ä»…ä¿ç•™å…³é”®è¡Œä¸Šä¸‹æ–‡)
function getSmartWindow(fullContent: string, targetLine: number, windowSize: number): string {
  const lines = fullContent.split('\n');
  const start = Math.max(0, targetLine - windowSize);
  const end = Math.min(lines.length, targetLine + windowSize);
  
  const snippet = lines.slice(start, end).join('\n');
  return `... (file truncated) ...\n${snippet}\n... (file truncated) ...`;
}
```

### 3. è¿™ä¸ªæ–¹æ¡ˆå¦‚ä½•æ»¡è¶³ä½ çš„éœ€æ±‚ï¼Ÿ

#### 1. é»˜è®¤ä¿è¯ "Maximum Stability"
åªè¦ Token å…è®¸ï¼Œç¨‹åºä¼šå§‹ç»ˆè¿›å…¥ `currentBatch.push({ content: fullContent })`ã€‚å¦‚æœä½ çš„æ¨¡å‹æœ‰ 128k æˆ– 200k ä¸Šä¸‹æ–‡ï¼ˆå¦‚ GPT-4-Turbo æˆ– Claude 3ï¼‰ï¼Œç»å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½ ä¼šä¸€æ¬¡æ€§æŠŠæ‰€æœ‰æ–‡ä»¶çš„**å…¨é‡å†…å®¹**å‘è¿‡å»ã€‚è¿™ç¬¦åˆä½  "Pre-fetch all" çš„åå¥½ã€‚

#### 2. è‡ªåŠ¨å¤„ç† "Overwhelming Context"
å½“æ–‡ä»¶å¤ªå¤šï¼ˆæ¯”å¦‚ä¸€ä¸‹å­å½±å“äº† 50 ä¸ªæ–‡ä»¶ï¼Œæˆ–è€…æœ‰å‡ ä¸ªå·¨å¤§çš„ 5000 è¡Œä»£ç æ–‡ä»¶ï¼‰ï¼Œ`nextTotal > AVAILABLE_TOKENS` åˆ¤æ–­æ¡ä»¶ä¼šè§¦å‘ã€‚
æ­¤æ—¶ç¨‹åº**ä¸ä¼š**å´©æºƒï¼Œä¹Ÿä¸ä¼šèƒ¡ä¹±ä¸¢å¼ƒæ–‡ä»¶ï¼Œè€Œæ˜¯è‡ªåŠ¨å°†ä»»åŠ¡**æ‹†åˆ† (Split)** æˆä¸¤ä¸ªæˆ–å¤šä¸ªè¯·æ±‚ã€‚
*   è¯·æ±‚ 1ï¼šåˆ†ææ–‡ä»¶ 1-10ï¼ˆå…¨é‡å†…å®¹ï¼‰
*   è¯·æ±‚ 2ï¼šåˆ†ææ–‡ä»¶ 11-20ï¼ˆå…¨é‡å†…å®¹ï¼‰

è¿™å¯¹ LLM æ¥è¯´æ›´å‹å¥½ï¼Œå› ä¸ºæ¯ä¸ªè¯·æ±‚çš„æ³¨æ„åŠ›æ›´é›†ä¸­ã€‚

#### 3. å…œåº•ä¿æŠ¤ (Fallback)
ä»£ç ä¸­çš„ `æƒ…å†µ A` æ˜¯ä¸ºäº†é˜²æ­¢å•ä¸ªæ–‡ä»¶æœ¬èº«å°±æ˜¯ä¸ªå‡ å…†çš„å·¨çŸ³ä»£ç ï¼ˆShit Mountainï¼‰ã€‚å¦‚æœä¸åšåˆ‡ç‰‡ï¼Œæ— è®ºæ€ä¹ˆåˆ†æ‰¹éƒ½å¡ä¸è¿›å»ã€‚åªæœ‰åœ¨è¿™ç§æç«¯æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ‰ç‰ºç‰²å…¨é‡å†…å®¹ï¼Œä½¿ç”¨ `Smart Window`ã€‚

### æ€»ç»“å»ºè®®

ä¸è¦è®© LLM å†³å®šåŠ è½½ä»€ä¹ˆï¼ˆä¸ç¨³å®šï¼‰ã€‚
**è®©ä»£ç åšæ•°å­¦é¢˜**ï¼š

1.  **é»˜è®¤åŠ¨ä½œ**ï¼šè¯»å–å®Œæ•´æ–‡ä»¶ã€‚
2.  **è®¡ç®—åŠ¨ä½œ**ï¼š`Diff Token` + `Files Token`ã€‚
3.  **å†³ç­–åŠ¨ä½œ**ï¼š
    *   `< Limit` ? å‘é€ä¸€ä¸ªè¯·æ±‚ã€‚
    *   `> Limit` ? æ‹†åˆ†æˆ `[Request 1, Request 2, ...]` å¹¶è¡Œå‘é€ã€‚

è¿™ç§**"åŸºäºé¢„ç®—çš„åˆ†æ‰¹å¤„ç†" (Budget-Based Batching)** æ˜¯ç›®å‰å¤§è§„æ¨¡ä»£ç åˆ†æä¸­æœ€ç¨³å¥çš„æ¨¡å¼ã€‚